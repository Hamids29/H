import re
import time
import unicodedata
import pandas as pd
import requests

BASE_URL = "https://npiregistry.cms.hhs.gov/api/"
LIMIT_PER_REQ = 200
MAX_PAGES = 6
TARGET_TAXONOMY = "General Acute Care Hospital"  # change if needed

# ---------- helpers ----------

def _norm(s: str) -> str:
    if not s: return ""
    s = unicodedata.normalize("NFKD", str(s)).encode("ascii", "ignore").decode("ascii")
    return re.sub(r"\s+", " ", s.lower()).strip()

def _primary_location_addr(addresses):
    return (addresses or [{}])[0] or {}

def _street_tokens(address1: str) -> set:
    """
    Turn '929 N SAINT FRANCIS AVE' / '929 N Saint Francis St' into tokens like:
    {'929','saint','francis'} â€” ignore suffixes (st/ave/rd), dirs (n/e/w/s), units.
    """
    s = _norm(address1)
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = s.replace(" st ", " saint ")  # normalize 'st' (as word) to 'saint'
    drops = {
        "street","st","saint","avenue","ave","road","rd","boulevard","blvd","lane","ln",
        "drive","dr","highway","hwy","terrace","ter","parkway","pkwy","place","pl",
        "court","ct","way","cir","circle",
        "n","s","e","w","ne","nw","se","sw","suite","ste","unit","apt","#"
    }
    toks = [t for t in s.split() if t and t not in drops]
    nums = [t for t in toks if t.isdigit()]
    alphas = [t for t in toks if not t.isdigit()]
    keep = set()
    if nums: keep.add(nums[0])          # house number
    if alphas:
        keep.add(alphas[0])             # first street word
        if len(alphas) > 1:
            keep.add(alphas[1])         # second street word (e.g., 'francis')
    return keep

def _addr_matches(addr_api: str, addr_input: str) -> bool:
    if not addr_api or not addr_input: return False
    api = _norm(addr_api)
    api = api.replace(" st ", " saint ")
    api = re.sub(r"[^a-z0-9\s]", " ", api)
    tokens = _street_tokens(addr_input)
    api_words = set(api.split())
    return all(tok in api_words for tok in tokens)

def _fetch_candidates(state: str, city: str | None) -> list:
    params = {
        "version": "2.1",
        "enumeration_type": "NPI-2",  # organizations
        "state": state,
        "limit": LIMIT_PER_REQ
    }
    if city: params["city"] = city

    all_results, skip = [], 0
    for _ in range(MAX_PAGES):
        params["skip"] = skip
        r = requests.get(BASE_URL, params=params, timeout=20)
        r.raise_for_status()
        data = r.json()
        batch = data.get("results", []) or []
        if not batch:
            break
        all_results.extend(batch)
        if len(batch) < params["limit"] or skip >= 1000:
            break
        skip += params["limit"]
        time.sleep(0.15)
    return all_results

def _has_taxonomy(r, target_taxonomy: str) -> bool:
    want = _norm(target_taxonomy)
    tax = { _norm(t.get("desc","")) for t in (r.get("taxonomies") or []) }
    return want in tax

def _choose_by_priority(cands_addr, cands_city, target_taxonomy: str) -> str:
    pools = [
        [r for r in cands_addr if _has_taxonomy(r, target_taxonomy)],  # address + taxonomy
        [r for r in cands_addr if not _has_taxonomy(r, target_taxonomy)],  # address + any
        [r for r in cands_city if _has_taxonomy(r, target_taxonomy)],  # city + taxonomy
        [r for r in cands_city if not _has_taxonomy(r, target_taxonomy)],  # city + any
    ]
    for pool in pools:
        if pool:
            return str(pool[0].get("number", "Not Found"))
    return "Not Found"

def resolve_npi(city: str, state: str, address1: str | None, target_taxonomy: str) -> str:
    n_city, n_state = _norm(city), _norm(state)
    # Fetch by state+city; if nothing, widen to state-only (some records have odd city fields)
    cands = _fetch_candidates(state, city)
    if not cands:
        cands = _fetch_candidates(state, None)

    # keep only those whose primary location matches state (+ city if provided)
    keep = []
    for r in cands:
        addr = _primary_location_addr(r.get("addresses", []))
        if _norm(addr.get("state")) != n_state: 
            continue
        if n_city and _norm(addr.get("city")) != n_city:
            continue
        keep.append(r)

    # split into address-matched vs city-only
    cands_addr, cands_city = [], []
    for r in keep:
        addr_api = _primary_location_addr(r.get("addresses", [])).get("address_1", "")
        if address1 and _addr_matches(addr_api, address1):
            cands_addr.append(r)
        else:
            cands_city.append(r)

    return _choose_by_priority(cands_addr, cands_city, target_taxonomy)

# ---------- batch over Excel ----------

input_filepath = "test_unknown_joint_knee_npi.xlsx"
output_filepath = "test_unknown_acuitymd_npi_here_2025_knee_surg.xlsx"

df = pd.read_excel(input_filepath)
if "NPI" not in df.columns:
    df["NPI"] = ""

# required columns: 'city', 'state'
# optional: 'address1' (recommended). We don't actually need 'name' now.
addr_col = "address1" if "address1" in df.columns else ("address_1" if "address_1" in df.columns else None)

for idx, row in df.iterrows():
    city = str(row.get("city", "")).strip()
    state = str(row.get("state", "")).strip()
    address1 = str(row.get(addr_col, "")).strip() if addr_col else ""

    npi = resolve_npi(city, state, address1, TARGET_TAXONOMY)
    df.at[idx, "NPI"] = npi
    print(f"Processed: {address1}, {city}, {state} -> {npi}")

df.to_excel(output_filepath, index=False)
print("Done! Saved to", output_filepath)



